{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>996009318</td>\n",
       "      <td>THE LIONS CLUB OF HONOLULU KAMEHAMEHA</td>\n",
       "      <td>T4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>996010315</td>\n",
       "      <td>INTERNATIONAL ASSOCIATION OF LIONS CLUBS</td>\n",
       "      <td>T4</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>996012607</td>\n",
       "      <td>PTA HAWAII CONGRESS</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>996015768</td>\n",
       "      <td>AMERICAN FEDERATION OF GOVERNMENT EMPLOYEES LO...</td>\n",
       "      <td>T5</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>996086871</td>\n",
       "      <td>WATERHOUSE CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1M-5M</td>\n",
       "      <td>N</td>\n",
       "      <td>36500179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34299 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             EIN                                               NAME  \\\n",
       "0       10520599                       BLUE KNIGHTS MOTORCYCLE CLUB   \n",
       "1       10531628             AMERICAN CHESAPEAKE CLUB CHARITABLE TR   \n",
       "2       10547893                 ST CLOUD PROFESSIONAL FIREFIGHTERS   \n",
       "3       10553066                     SOUTHSIDE ATHLETIC ASSOCIATION   \n",
       "4       10556103           GENETIC RESEARCH INSTITUTE OF THE DESERT   \n",
       "...          ...                                                ...   \n",
       "34294  996009318              THE LIONS CLUB OF HONOLULU KAMEHAMEHA   \n",
       "34295  996010315           INTERNATIONAL ASSOCIATION OF LIONS CLUBS   \n",
       "34296  996012607                                PTA HAWAII CONGRESS   \n",
       "34297  996015768  AMERICAN FEDERATION OF GOVERNMENT EMPLOYEES LO...   \n",
       "34298  996086871                           WATERHOUSE CHARITABLE TR   \n",
       "\n",
       "      APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0                  T10       Independent          C1000    ProductDev   \n",
       "1                   T3       Independent          C2000  Preservation   \n",
       "2                   T5  CompanySponsored          C3000    ProductDev   \n",
       "3                   T3  CompanySponsored          C2000  Preservation   \n",
       "4                   T3       Independent          C1000     Heathcare   \n",
       "...                ...               ...            ...           ...   \n",
       "34294               T4       Independent          C1000    ProductDev   \n",
       "34295               T4  CompanySponsored          C3000    ProductDev   \n",
       "34296               T3  CompanySponsored          C2000  Preservation   \n",
       "34297               T5       Independent          C3000    ProductDev   \n",
       "34298               T3       Independent          C1000  Preservation   \n",
       "\n",
       "       ORGANIZATION  STATUS     INCOME_AMT SPECIAL_CONSIDERATIONS   ASK_AMT  \\\n",
       "0       Association       1              0                      N      5000   \n",
       "1      Co-operative       1         1-9999                      N    108590   \n",
       "2       Association       1              0                      N      5000   \n",
       "3             Trust       1    10000-24999                      N      6692   \n",
       "4             Trust       1  100000-499999                      N    142590   \n",
       "...             ...     ...            ...                    ...       ...   \n",
       "34294   Association       1              0                      N      5000   \n",
       "34295   Association       1              0                      N      5000   \n",
       "34296   Association       1              0                      N      5000   \n",
       "34297   Association       1              0                      N      5000   \n",
       "34298  Co-operative       1          1M-5M                      N  36500179   \n",
       "\n",
       "       IS_SUCCESSFUL  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  0  \n",
       "3                  1  \n",
       "4                  1  \n",
       "...              ...  \n",
       "34294              0  \n",
       "34295              0  \n",
       "34296              0  \n",
       "34297              1  \n",
       "34298              0  \n",
       "\n",
       "[34299 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our input dataset\n",
    "charity_df = pd.read_csv('charity_data.csv')\n",
    "charity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column EIN has 0 null values\n",
      "Column NAME has 0 null values\n",
      "Column APPLICATION_TYPE has 0 null values\n",
      "Column AFFILIATION has 0 null values\n",
      "Column CLASSIFICATION has 0 null values\n",
      "Column USE_CASE has 0 null values\n",
      "Column ORGANIZATION has 0 null values\n",
      "Column STATUS has 0 null values\n",
      "Column INCOME_AMT has 0 null values\n",
      "Column SPECIAL_CONSIDERATIONS has 0 null values\n",
      "Column ASK_AMT has 0 null values\n",
      "Column IS_SUCCESSFUL has 0 null values\n"
     ]
    }
   ],
   "source": [
    "# Checking null values\n",
    "for column in charity_df.columns:\n",
    "    print(f\"Column {column} has {charity_df[column].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIN                        int64\n",
       "NAME                      object\n",
       "APPLICATION_TYPE          object\n",
       "AFFILIATION               object\n",
       "CLASSIFICATION            object\n",
       "USE_CASE                  object\n",
       "ORGANIZATION              object\n",
       "STATUS                     int64\n",
       "INCOME_AMT                object\n",
       "SPECIAL_CONSIDERATIONS    object\n",
       "ASK_AMT                    int64\n",
       "IS_SUCCESSFUL              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charity_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1-9999', '10000-24999', '100000-499999', '10M-50M',\n",
       "       '25000-99999', '50M+', '1M-5M', '5M-10M'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charity_df[\"INCOME_AMT\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T10', 'T3', 'T5', 'T7', 'T4', 'T6', 'T2', 'T9', 'T19', 'T8',\n",
       "       'T13', 'T12', 'T29', 'T25', 'T14', 'T17', 'T15'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charity_df[\"APPLICATION_TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIN                       34299\n",
       "NAME                      19568\n",
       "APPLICATION_TYPE             17\n",
       "AFFILIATION                   6\n",
       "CLASSIFICATION               71\n",
       "USE_CASE                      5\n",
       "ORGANIZATION                  4\n",
       "STATUS                        2\n",
       "INCOME_AMT                    9\n",
       "SPECIAL_CONSIDERATIONS        2\n",
       "ASK_AMT                    8747\n",
       "IS_SUCCESSFUL                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charity_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C1000', 'C2000', 'C3000', 'C1200', 'C2700', 'C7000', 'C7200',\n",
       "       'C1700', 'C4000', 'C7100', 'C2800', 'C6000', 'C2100', 'C1238',\n",
       "       'C5000', 'C7120', 'C1800', 'C4100', 'C1400', 'C1270', 'C2300',\n",
       "       'C8200', 'C1500', 'C7210', 'C1300', 'C1230', 'C1280', 'C1240',\n",
       "       'C2710', 'C2561', 'C1250', 'C8000', 'C1245', 'C1260', 'C1235',\n",
       "       'C1720', 'C1257', 'C4500', 'C2400', 'C8210', 'C1600', 'C1278',\n",
       "       'C1237', 'C4120', 'C2170', 'C1728', 'C1732', 'C2380', 'C1283',\n",
       "       'C1570', 'C2500', 'C1267', 'C3700', 'C1580', 'C2570', 'C1256',\n",
       "       'C1236', 'C1234', 'C1246', 'C2190', 'C4200', 'C0', 'C3200',\n",
       "       'C5200', 'C1370', 'C2600', 'C1248', 'C6100', 'C1820', 'C1900',\n",
       "       'C2150'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charity_df[\"CLASSIFICATION\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion it doesn't make sense to apply binning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the df to two - categorical and numerical, defininf the target and features, and dropping the columns that are neither "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>T4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>T4</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>T5</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1M-5M</td>\n",
       "      <td>N</td>\n",
       "      <td>36500179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34299 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0                  T10       Independent          C1000    ProductDev   \n",
       "1                   T3       Independent          C2000  Preservation   \n",
       "2                   T5  CompanySponsored          C3000    ProductDev   \n",
       "3                   T3  CompanySponsored          C2000  Preservation   \n",
       "4                   T3       Independent          C1000     Heathcare   \n",
       "...                ...               ...            ...           ...   \n",
       "34294               T4       Independent          C1000    ProductDev   \n",
       "34295               T4  CompanySponsored          C3000    ProductDev   \n",
       "34296               T3  CompanySponsored          C2000  Preservation   \n",
       "34297               T5       Independent          C3000    ProductDev   \n",
       "34298               T3       Independent          C1000  Preservation   \n",
       "\n",
       "       ORGANIZATION  STATUS     INCOME_AMT SPECIAL_CONSIDERATIONS   ASK_AMT  \\\n",
       "0       Association       1              0                      N      5000   \n",
       "1      Co-operative       1         1-9999                      N    108590   \n",
       "2       Association       1              0                      N      5000   \n",
       "3             Trust       1    10000-24999                      N      6692   \n",
       "4             Trust       1  100000-499999                      N    142590   \n",
       "...             ...     ...            ...                    ...       ...   \n",
       "34294   Association       1              0                      N      5000   \n",
       "34295   Association       1              0                      N      5000   \n",
       "34296   Association       1              0                      N      5000   \n",
       "34297   Association       1              0                      N      5000   \n",
       "34298  Co-operative       1          1M-5M                      N  36500179   \n",
       "\n",
       "       IS_SUCCESSFUL  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  0  \n",
       "3                  1  \n",
       "4                  1  \n",
       "...              ...  \n",
       "34294              0  \n",
       "34295              0  \n",
       "34296              0  \n",
       "34297              1  \n",
       "34298              0  \n",
       "\n",
       "[34299 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping redundant data. \n",
    "# As I think the identification columns such as EIN and NAME \n",
    "# are not making any features in our dataset\n",
    "charity_reduced_df = charity_df.drop(columns=[\"EIN\",\"NAME\"], axis=1)\n",
    "charity_reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STATUS', 'ASK_AMT', 'IS_SUCCESSFUL']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a numeric df\n",
    "numeric_columns = charity_reduced_df.dtypes[charity_reduced_df.dtypes == \"int64\"].index.tolist()\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>1</td>\n",
       "      <td>36500179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34299 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATUS   ASK_AMT\n",
       "0           1      5000\n",
       "1           1    108590\n",
       "2           1      5000\n",
       "3           1      6692\n",
       "4           1    142590\n",
       "...       ...       ...\n",
       "34294       1      5000\n",
       "34295       1      5000\n",
       "34296       1      5000\n",
       "34297       1      5000\n",
       "34298       1  36500179\n",
       "\n",
       "[34299 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping only the 'STATUS' and 'ASK_AMT'. \n",
    "# The 'IS_SUCCESSFUL' will be the target\n",
    "numeric_columns_df = charity_reduced_df[['STATUS', 'ASK_AMT']]\n",
    "numeric_columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APPLICATION_TYPE',\n",
       " 'AFFILIATION',\n",
       " 'CLASSIFICATION',\n",
       " 'USE_CASE',\n",
       " 'ORGANIZATION',\n",
       " 'INCOME_AMT',\n",
       " 'SPECIAL_CONSIDERATIONS']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a categorical df - features that have to be encoded\n",
    "categ_columns = charity_reduced_df.dtypes[charity_reduced_df.dtypes == \"object\"].index.tolist()\n",
    "categ_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>T4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>T4</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Association</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>T5</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1M-5M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34299 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0                  T10       Independent          C1000    ProductDev   \n",
       "1                   T3       Independent          C2000  Preservation   \n",
       "2                   T5  CompanySponsored          C3000    ProductDev   \n",
       "3                   T3  CompanySponsored          C2000  Preservation   \n",
       "4                   T3       Independent          C1000     Heathcare   \n",
       "...                ...               ...            ...           ...   \n",
       "34294               T4       Independent          C1000    ProductDev   \n",
       "34295               T4  CompanySponsored          C3000    ProductDev   \n",
       "34296               T3  CompanySponsored          C2000  Preservation   \n",
       "34297               T5       Independent          C3000    ProductDev   \n",
       "34298               T3       Independent          C1000  Preservation   \n",
       "\n",
       "       ORGANIZATION     INCOME_AMT SPECIAL_CONSIDERATIONS  \n",
       "0       Association              0                      N  \n",
       "1      Co-operative         1-9999                      N  \n",
       "2       Association              0                      N  \n",
       "3             Trust    10000-24999                      N  \n",
       "4             Trust  100000-499999                      N  \n",
       "...             ...            ...                    ...  \n",
       "34294   Association              0                      N  \n",
       "34295   Association              0                      N  \n",
       "34296   Association              0                      N  \n",
       "34297   Association              0                      N  \n",
       "34298  Co-operative          1M-5M                      N  \n",
       "\n",
       "[34299 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categ_columns_df = charity_reduced_df[['APPLICATION_TYPE',\n",
    " 'AFFILIATION',\n",
    " 'CLASSIFICATION',\n",
    " 'USE_CASE',\n",
    " 'ORGANIZATION',\n",
    " 'INCOME_AMT',\n",
    " 'SPECIAL_CONSIDERATIONS']]\n",
    "categ_columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "34294    0\n",
       "34295    0\n",
       "34296    0\n",
       "34297    1\n",
       "34298    0\n",
       "Name: IS_SUCCESSFUL, Length: 34299, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the target\n",
    "y = charity_reduced_df['IS_SUCCESSFUL']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34299 rows Ã— 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  104  105  106  \\\n",
       "0      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "34294  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "34295  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "34296  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "34297  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "34298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       107  108  109  110  111  112  113  \n",
       "0      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "34294  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "34295  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "34296  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "34297  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "34298  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[34299 rows x 114 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the categorical features\n",
    "\n",
    "# Creating a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fitting and transforming the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(categ_columns_df))\n",
    "encode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  \\\n",
       "0                   0.0  ...                0.0                     0.0   \n",
       "1                   0.0  ...                1.0                     0.0   \n",
       "2                   0.0  ...                0.0                     0.0   \n",
       "3                   0.0  ...                0.0                     1.0   \n",
       "4                   0.0  ...                0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  \\\n",
       "0                       0.0                 0.0               0.0   \n",
       "1                       0.0                 0.0               0.0   \n",
       "2                       0.0                 0.0               0.0   \n",
       "3                       0.0                 0.0               0.0   \n",
       "4                       1.0                 0.0               0.0   \n",
       "\n",
       "   INCOME_AMT_25000-99999  INCOME_AMT_50M+  INCOME_AMT_5M-10M  \\\n",
       "0                     0.0              0.0                0.0   \n",
       "1                     0.0              0.0                0.0   \n",
       "2                     0.0              0.0                0.0   \n",
       "3                     0.0              0.0                0.0   \n",
       "4                     0.0              0.0                0.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       1.0                       0.0  \n",
       "1                       1.0                       0.0  \n",
       "2                       1.0                       0.0  \n",
       "3                       1.0                       0.0  \n",
       "4                       1.0                       0.0  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(categ_columns)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  ASK_AMT  APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  \\\n",
       "0       1     5000                   1.0                   0.0   \n",
       "1       1   108590                   0.0                   0.0   \n",
       "2       1     5000                   0.0                   0.0   \n",
       "3       1     6692                   0.0                   0.0   \n",
       "4       1   142590                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T13  APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T17  APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  ...  \\\n",
       "0                   0.0                   0.0                  0.0  ...   \n",
       "1                   0.0                   0.0                  0.0  ...   \n",
       "2                   0.0                   0.0                  0.0  ...   \n",
       "3                   0.0                   0.0                  0.0  ...   \n",
       "4                   0.0                   0.0                  0.0  ...   \n",
       "\n",
       "   INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "0                0.0                     0.0                       0.0   \n",
       "1                1.0                     0.0                       0.0   \n",
       "2                0.0                     0.0                       0.0   \n",
       "3                0.0                     1.0                       0.0   \n",
       "4                0.0                     0.0                       1.0   \n",
       "\n",
       "   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "0                 0.0               0.0                     0.0   \n",
       "1                 0.0               0.0                     0.0   \n",
       "2                 0.0               0.0                     0.0   \n",
       "3                 0.0               0.0                     0.0   \n",
       "4                 0.0               0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "0              0.0                0.0                       1.0   \n",
       "1              0.0                0.0                       1.0   \n",
       "2              0.0                0.0                       1.0   \n",
       "3              0.0                0.0                       1.0   \n",
       "4              0.0                0.0                       1.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging encoded categorical data with numerical df\n",
    "X = numeric_columns_df.merge(encode_df, left_index=True, right_index=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the features dataset has to be split to training and testing data, and then to be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X_train and X_test\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01394306, -0.03348439, -0.12647487, ..., -0.06987849,\n",
       "         0.02789425, -0.02789425],\n",
       "       [ 0.01394306, -0.03348439, -0.12647487, ..., -0.06987849,\n",
       "         0.02789425, -0.02789425],\n",
       "       [ 0.01394306, -0.03348439, -0.12647487, ..., -0.06987849,\n",
       "         0.02789425, -0.02789425],\n",
       "       ...,\n",
       "       [ 0.01394306, -0.03229417, -0.12647487, ..., -0.06987849,\n",
       "         0.02789425, -0.02789425],\n",
       "       [ 0.01394306, -0.03348439, -0.12647487, ..., -0.06987849,\n",
       "         0.02789425, -0.02789425],\n",
       "       [ 0.01394306, -0.01093629, -0.12647487, ..., -0.06987849,\n",
       "         0.02789425, -0.02789425]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.39430557e-02, -3.33676580e-02, -1.26474869e-01, ...,\n",
       "        -6.98784891e-02,  2.78942470e-02, -2.78942470e-02],\n",
       "       [ 1.39430557e-02,  2.86802949e-02, -1.26474869e-01, ...,\n",
       "         1.43105555e+01,  2.78942470e-02, -2.78942470e-02],\n",
       "       [ 1.39430557e-02, -3.34843874e-02, -1.26474869e-01, ...,\n",
       "        -6.98784891e-02,  2.78942470e-02, -2.78942470e-02],\n",
       "       ...,\n",
       "       [ 1.39430557e-02, -3.34843874e-02, -1.26474869e-01, ...,\n",
       "        -6.98784891e-02,  2.78942470e-02, -2.78942470e-02],\n",
       "       [ 1.39430557e-02, -3.34843874e-02, -1.26474869e-01, ...,\n",
       "        -6.98784891e-02,  2.78942470e-02, -2.78942470e-02],\n",
       "       [ 1.39430557e-02, -3.34843874e-02, -1.26474869e-01, ...,\n",
       "        -6.98784891e-02,  2.78942470e-02, -2.78942470e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Will start with the Tanh activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, will use one layer with 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 =  10\n",
    "# hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 36us/sample - loss: 0.5909 - acc: 0.7098\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 1s 34us/sample - loss: 0.5660 - acc: 0.7263\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 1s 36us/sample - loss: 0.5589 - acc: 0.7289\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 1s 35us/sample - loss: 0.5540 - acc: 0.7317\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 1s 35us/sample - loss: 0.5513 - acc: 0.7326\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 1s 36us/sample - loss: 0.5490 - acc: 0.7337\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5479 - acc: 0.7336\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5466 - acc: 0.7350\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 35us/sample - loss: 0.5455 - acc: 0.7345\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 1s 34us/sample - loss: 0.5450 - acc: 0.7354\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 1s 34us/sample - loss: 0.5443 - acc: 0.7351\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 1s 33us/sample - loss: 0.5440 - acc: 0.7356\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5435 - acc: 0.7360\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5431 - acc: 0.7360\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5424 - acc: 0.7365\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5424 - acc: 0.7374\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5420 - acc: 0.7364\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5415 - acc: 0.7364\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5415 - acc: 0.7369\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5411 - acc: 0.7362\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5408 - acc: 0.7373\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5409 - acc: 0.7367\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5405 - acc: 0.7376\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5404 - acc: 0.7367\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5401 - acc: 0.7369\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5405 - acc: 0.7379\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5402 - acc: 0.73720s - loss: 0.\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5397 - acc: 0.7384\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5396 - acc: 0.7377\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5396 - acc: 0.7367\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5395 - acc: 0.7382\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5396 - acc: 0.7374\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5393 - acc: 0.7389\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5391 - acc: 0.7387\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5388 - acc: 0.7385\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5387 - acc: 0.7381\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5389 - acc: 0.7384\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5387 - acc: 0.7383\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5385 - acc: 0.7385\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5385 - acc: 0.7392\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5385 - acc: 0.7385\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5383 - acc: 0.7393\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5380 - acc: 0.7385\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5380 - acc: 0.7392\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5380 - acc: 0.7392\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5377 - acc: 0.7388\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5376 - acc: 0.7394\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5375 - acc: 0.7392\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5375 - acc: 0.7395\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5374 - acc: 0.7395\n",
      "8575/8575 - 0s - loss: 0.5532 - acc: 0.7263\n",
      "Loss: 0.5532350846977345, Accuracy: 0.7262973785400391\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 50 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is very high and the accuracy level is not impressive. So I would like to tyr increasing the number of neurones upto twice as much inputs we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "# hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5836 - acc: 0.7178\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5686 - acc: 0.7287\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5590 - acc: 0.7317\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5530 - acc: 0.7313\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5500 - acc: 0.7328\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5471 - acc: 0.7336\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5461 - acc: 0.7328\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5445 - acc: 0.7352\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5444 - acc: 0.7338\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5430 - acc: 0.7350\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5424 - acc: 0.7345\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5420 - acc: 0.7351\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5415 - acc: 0.7350\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5410 - acc: 0.7359\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5409 - acc: 0.7347\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5402 - acc: 0.7361\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5404 - acc: 0.7362\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5396 - acc: 0.7367\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5388 - acc: 0.7362\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5391 - acc: 0.7374\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5391 - acc: 0.7353\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5381 - acc: 0.7380\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5376 - acc: 0.7390\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5380 - acc: 0.7371\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5378 - acc: 0.7382\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5376 - acc: 0.7375\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5372 - acc: 0.7390\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5371 - acc: 0.7383\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5369 - acc: 0.7376\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5367 - acc: 0.7386\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5361 - acc: 0.7379\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5365 - acc: 0.7374\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5360 - acc: 0.7403\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5362 - acc: 0.7381\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5353 - acc: 0.7408\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5355 - acc: 0.7400\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5357 - acc: 0.7390\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5351 - acc: 0.7390\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5352 - acc: 0.7408\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5356 - acc: 0.7391\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5350 - acc: 0.7402\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5345 - acc: 0.7404\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5348 - acc: 0.7393\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5344 - acc: 0.7398\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5343 - acc: 0.7416\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5345 - acc: 0.7414\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5344 - acc: 0.7413\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5343 - acc: 0.74211s - lo\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5342 - acc: 0.7415\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5340 - acc: 0.7418\n",
      "8575/8575 - 0s - loss: 0.5562 - acc: 0.7273\n",
      "Loss: 0.5561871367015226, Accuracy: 0.7273469567298889\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 50 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no improvement at all. I will increase the number of epochs but will apply it to the first case - with less neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 =  10\n",
    "# hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5875 - acc: 0.7105\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5666 - acc: 0.7269\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5610 - acc: 0.7289\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5567 - acc: 0.7323\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5533 - acc: 0.7314\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5503 - acc: 0.7336\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5486 - acc: 0.7349\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5475 - acc: 0.7342\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5461 - acc: 0.7351\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5456 - acc: 0.7350\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5447 - acc: 0.7361\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5443 - acc: 0.7357\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5435 - acc: 0.7358\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5433 - acc: 0.7363\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5430 - acc: 0.7363\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5427 - acc: 0.7366\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5419 - acc: 0.7363\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5418 - acc: 0.7369\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5416 - acc: 0.7366\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5416 - acc: 0.7374\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5415 - acc: 0.7372\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5407 - acc: 0.7380\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5408 - acc: 0.7365\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5407 - acc: 0.7378\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5404 - acc: 0.7380\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5403 - acc: 0.7376\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5401 - acc: 0.7374\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5400 - acc: 0.7383\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5398 - acc: 0.7380\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5401 - acc: 0.7383\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5400 - acc: 0.7385\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5398 - acc: 0.7380\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5394 - acc: 0.7383\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5395 - acc: 0.7386\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5391 - acc: 0.7395\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5391 - acc: 0.7383\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5390 - acc: 0.7393\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5390 - acc: 0.7385\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5389 - acc: 0.7385\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5388 - acc: 0.7381\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5384 - acc: 0.7383\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5383 - acc: 0.7386\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5386 - acc: 0.7385\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5382 - acc: 0.73830s - loss:\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5383 - acc: 0.7377\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5384 - acc: 0.7379\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5383 - acc: 0.7371\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5381 - acc: 0.7386\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5380 - acc: 0.7382\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5382 - acc: 0.7377\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5379 - acc: 0.7389\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5377 - acc: 0.7378\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5377 - acc: 0.7378\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5377 - acc: 0.7384\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5378 - acc: 0.7386\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5377 - acc: 0.7387\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5376 - acc: 0.7389\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5375 - acc: 0.7377\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5376 - acc: 0.7388\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5371 - acc: 0.7390\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5375 - acc: 0.7384\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5372 - acc: 0.7393\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5373 - acc: 0.7383\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5371 - acc: 0.7396\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5370 - acc: 0.7384\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5371 - acc: 0.7393\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5371 - acc: 0.7385\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5371 - acc: 0.7396\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5366 - acc: 0.7392\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5370 - acc: 0.7393\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5370 - acc: 0.7388\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5368 - acc: 0.7398\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5370 - acc: 0.7401\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5368 - acc: 0.7397\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5367 - acc: 0.7402\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5367 - acc: 0.7390\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5365 - acc: 0.7383\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5367 - acc: 0.7395\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5368 - acc: 0.7393\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5369 - acc: 0.7393\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5364 - acc: 0.7399\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5366 - acc: 0.7397\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5365 - acc: 0.7399\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5365 - acc: 0.7395\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5365 - acc: 0.7391\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5364 - acc: 0.7396\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5366 - acc: 0.7398\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5364 - acc: 0.7390\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5364 - acc: 0.7401\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5362 - acc: 0.7385\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5362 - acc: 0.7396\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5363 - acc: 0.7400\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5362 - acc: 0.7394\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5361 - acc: 0.7404\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5363 - acc: 0.7401\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5360 - acc: 0.7395\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5361 - acc: 0.7398\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5361 - acc: 0.7406\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5359 - acc: 0.7407\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5359 - acc: 0.7406\n",
      "8575/8575 - 0s - loss: 0.5505 - acc: 0.7266\n",
      "Loss: 0.5505023111268313, Accuracy: 0.7266472578048706\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result id not better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use the combination of 100 epochs and number of neurons equal to double size of inputs. If it doesn't work will move to an additional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "# hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5843 - acc: 0.7159\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5677 - acc: 0.7272\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5599 - acc: 0.7296\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5528 - acc: 0.7316\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5498 - acc: 0.7324\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5480 - acc: 0.7324\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5463 - acc: 0.7338\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5449 - acc: 0.7348\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5436 - acc: 0.7357\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5427 - acc: 0.7360\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5428 - acc: 0.7346\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5416 - acc: 0.7358\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5410 - acc: 0.7357\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5409 - acc: 0.7367\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5404 - acc: 0.7372\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5401 - acc: 0.7375\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5394 - acc: 0.7378\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5396 - acc: 0.7372\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5390 - acc: 0.7379\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5386 - acc: 0.7373\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5387 - acc: 0.7377\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5381 - acc: 0.7385\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5377 - acc: 0.7374\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5375 - acc: 0.7391\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5377 - acc: 0.7379\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5372 - acc: 0.7393\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5368 - acc: 0.7381\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5369 - acc: 0.7394\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5365 - acc: 0.7390\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5363 - acc: 0.7395\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5364 - acc: 0.7391\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5357 - acc: 0.7386\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5357 - acc: 0.7383\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5359 - acc: 0.7414\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5357 - acc: 0.7400\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5356 - acc: 0.7386\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5353 - acc: 0.7398\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5355 - acc: 0.7416\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5348 - acc: 0.7407\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5345 - acc: 0.7413\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5344 - acc: 0.7406\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5345 - acc: 0.7401\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5345 - acc: 0.7399\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5346 - acc: 0.7409\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5342 - acc: 0.7413\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5339 - acc: 0.7413\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5340 - acc: 0.7409\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5339 - acc: 0.7421\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5337 - acc: 0.7402\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5331 - acc: 0.7412\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5341 - acc: 0.7408\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5332 - acc: 0.7427\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5334 - acc: 0.7396\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5336 - acc: 0.7415\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5335 - acc: 0.7410\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5329 - acc: 0.7419\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5332 - acc: 0.7414\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5328 - acc: 0.7418\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5331 - acc: 0.7426\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5327 - acc: 0.7426\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5326 - acc: 0.7411\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5322 - acc: 0.7413\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5324 - acc: 0.7414\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5322 - acc: 0.7427\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5320 - acc: 0.7423\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5327 - acc: 0.7408\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5325 - acc: 0.7409\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5324 - acc: 0.7423\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5321 - acc: 0.7416\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5316 - acc: 0.7413\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5323 - acc: 0.7423\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5321 - acc: 0.7401\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5318 - acc: 0.7427\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5317 - acc: 0.7430\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5313 - acc: 0.7427\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5318 - acc: 0.7424\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5313 - acc: 0.7406\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5314 - acc: 0.7421\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5313 - acc: 0.7424\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5311 - acc: 0.7435\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5311 - acc: 0.7430\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5309 - acc: 0.7428\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5312 - acc: 0.7424\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5315 - acc: 0.7418\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5310 - acc: 0.7428\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5311 - acc: 0.7426\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5314 - acc: 0.7426\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5308 - acc: 0.7417\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5304 - acc: 0.7431\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5309 - acc: 0.7415\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5308 - acc: 0.7413\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5308 - acc: 0.7422\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5309 - acc: 0.7420\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5304 - acc: 0.7426\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5304 - acc: 0.7431\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5308 - acc: 0.7424\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5307 - acc: 0.7430\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5306 - acc: 0.7427\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5309 - acc: 0.7427\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5305 - acc: 0.7425\n",
      "8575/8575 - 0s - loss: 0.5569 - acc: 0.7261\n",
      "Loss: 0.5569498400020877, Accuracy: 0.726064145565033\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's not better again. Time to try one more layer. If it doesn't improve the result then I will rty the RELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 10\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"tanh\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5826 - acc: 0.7130\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5595 - acc: 0.7306\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5546 - acc: 0.7324\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5520 - acc: 0.7345\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5504 - acc: 0.7341\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5492 - acc: 0.7349\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5483 - acc: 0.7339\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5475 - acc: 0.7344\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5468 - acc: 0.7348\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5462 - acc: 0.7345\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5456 - acc: 0.7353\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5451 - acc: 0.7349\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5445 - acc: 0.7361\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5443 - acc: 0.7364\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5437 - acc: 0.7359\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5435 - acc: 0.7361\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5433 - acc: 0.7367\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5428 - acc: 0.7368\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5423 - acc: 0.7369\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5422 - acc: 0.7369\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5419 - acc: 0.7369\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5417 - acc: 0.7365\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5412 - acc: 0.7374\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5412 - acc: 0.7372\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5409 - acc: 0.7371\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5408 - acc: 0.7385\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5405 - acc: 0.7379\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5403 - acc: 0.7378\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5402 - acc: 0.7381\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5401 - acc: 0.7384\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5399 - acc: 0.7379\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5397 - acc: 0.7386\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5395 - acc: 0.7386\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5392 - acc: 0.7385\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5392 - acc: 0.7386\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5390 - acc: 0.7390\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5390 - acc: 0.7385\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5389 - acc: 0.7398\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5387 - acc: 0.7388\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5385 - acc: 0.7390\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5385 - acc: 0.7391\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5383 - acc: 0.7385\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5382 - acc: 0.7385\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5380 - acc: 0.7385\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5380 - acc: 0.7388\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5381 - acc: 0.7392\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5380 - acc: 0.7388\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5379 - acc: 0.7389\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5376 - acc: 0.7387\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5376 - acc: 0.7393\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5377 - acc: 0.7392\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5375 - acc: 0.7394\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5376 - acc: 0.7392\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5373 - acc: 0.7393\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5373 - acc: 0.7396\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5372 - acc: 0.7395\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5371 - acc: 0.7401\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5370 - acc: 0.7396\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5370 - acc: 0.7393\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5371 - acc: 0.7394\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5367 - acc: 0.7401\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5366 - acc: 0.7399\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5368 - acc: 0.7400\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5370 - acc: 0.7400\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5366 - acc: 0.7404\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5366 - acc: 0.7394\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5367 - acc: 0.7408\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5361 - acc: 0.7405\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5366 - acc: 0.7401\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5364 - acc: 0.7400\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5364 - acc: 0.7400\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5366 - acc: 0.7395\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5364 - acc: 0.7407\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5360 - acc: 0.7395\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5363 - acc: 0.7395\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5363 - acc: 0.7403\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5361 - acc: 0.7400\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5361 - acc: 0.7398\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5360 - acc: 0.7398\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5361 - acc: 0.7400\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5360 - acc: 0.7397\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5359 - acc: 0.7394\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5360 - acc: 0.7396\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5357 - acc: 0.7409\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5359 - acc: 0.7397\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5357 - acc: 0.7402\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5359 - acc: 0.7400\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5358 - acc: 0.7404\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5357 - acc: 0.7404\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5356 - acc: 0.7406\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5356 - acc: 0.7406\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5356 - acc: 0.7403\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5356 - acc: 0.7395\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5357 - acc: 0.7403\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5355 - acc: 0.7392\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5355 - acc: 0.7406\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5357 - acc: 0.7400\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5354 - acc: 0.7399\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5355 - acc: 0.7400\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5352 - acc: 0.7404\n",
      "8575/8575 - 0s - loss: 0.5529 - acc: 0.7263\n",
      "Loss: 0.5528642978890644, Accuracy: 0.7262973785400391\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not satisfying again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RELU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will start with one layer and 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "# hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5767 - acc: 0.7245\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5612 - acc: 0.7302\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5546 - acc: 0.7320\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5533 - acc: 0.7322\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5499 - acc: 0.7328\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5510 - acc: 0.7330\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5483 - acc: 0.7320\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5480 - acc: 0.7334\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5477 - acc: 0.7343\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5465 - acc: 0.7334\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5475 - acc: 0.7343\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5452 - acc: 0.7350\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5487 - acc: 0.7343\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5444 - acc: 0.7343\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5435 - acc: 0.7353\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5439 - acc: 0.7350\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5438 - acc: 0.7346\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5428 - acc: 0.7357\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5424 - acc: 0.7362\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5448 - acc: 0.7366\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5425 - acc: 0.7362\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5419 - acc: 0.7365\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5420 - acc: 0.7352\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5418 - acc: 0.7358\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5414 - acc: 0.7360\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5474 - acc: 0.7365\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5405 - acc: 0.7376\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5399 - acc: 0.7376\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5400 - acc: 0.7374\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5402 - acc: 0.7370\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5478 - acc: 0.7371\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5392 - acc: 0.7374\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5396 - acc: 0.7377\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5391 - acc: 0.7382\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5392 - acc: 0.7377\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5396 - acc: 0.7374\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5378 - acc: 0.7375\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5398 - acc: 0.7369\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5377 - acc: 0.7385\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5387 - acc: 0.7377\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5383 - acc: 0.7393\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5432 - acc: 0.7376\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5384 - acc: 0.7387\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5370 - acc: 0.7385\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5376 - acc: 0.7388\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5375 - acc: 0.7385\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5372 - acc: 0.7395\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5375 - acc: 0.7385\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5553 - acc: 0.7390\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5378 - acc: 0.7383\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5365 - acc: 0.7392\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5369 - acc: 0.7393\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5366 - acc: 0.7376\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5369 - acc: 0.7390\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5365 - acc: 0.7387\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5365 - acc: 0.7403\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5733 - acc: 0.7377\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5364 - acc: 0.7396\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5360 - acc: 0.7404\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5357 - acc: 0.7400\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5362 - acc: 0.7398\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5362 - acc: 0.7409\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5356 - acc: 0.7394\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5361 - acc: 0.7397\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5358 - acc: 0.7389\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5355 - acc: 0.7395\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5543 - acc: 0.7397\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5351 - acc: 0.7406\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5353 - acc: 0.7406\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5351 - acc: 0.7397\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5347 - acc: 0.7409\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5354 - acc: 0.7413\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5353 - acc: 0.7405\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5580 - acc: 0.7402\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5348 - acc: 0.7408\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5344 - acc: 0.7397\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5350 - acc: 0.7418\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5344 - acc: 0.7392\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5345 - acc: 0.7411\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5346 - acc: 0.7410\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5636 - acc: 0.7397\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5342 - acc: 0.7403\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5344 - acc: 0.7408\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5343 - acc: 0.7404\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5340 - acc: 0.7409\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5335 - acc: 0.7418\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5557 - acc: 0.7401\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5345 - acc: 0.7422\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5334 - acc: 0.7414\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5341 - acc: 0.7406\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5333 - acc: 0.7412\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5345 - acc: 0.7400\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5458 - acc: 0.7421\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5413 - acc: 0.7414\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5332 - acc: 0.7411\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5334 - acc: 0.7417\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5327 - acc: 0.7407\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5336 - acc: 0.7416\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5329 - acc: 0.7414\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5328 - acc: 0.7408\n",
      "8575/8575 - 0s - loss: 0.6170 - acc: 0.7296\n",
      "Loss: 0.6170308780878695, Accuracy: 0.7295626997947693\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding one more layer but with less neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 10\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.6150 - acc: 0.6751\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5628 - acc: 0.7230\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5534 - acc: 0.7293\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5503 - acc: 0.7308\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5486 - acc: 0.7291\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5476 - acc: 0.7310\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5466 - acc: 0.7315\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5458 - acc: 0.7329\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5451 - acc: 0.7328\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5446 - acc: 0.7341\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5441 - acc: 0.7343\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5441 - acc: 0.7335\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5439 - acc: 0.7345\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5433 - acc: 0.7356\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5427 - acc: 0.7349\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5426 - acc: 0.7353\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5427 - acc: 0.7356\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5423 - acc: 0.7352\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5422 - acc: 0.7355\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5421 - acc: 0.7358\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5416 - acc: 0.7363\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5417 - acc: 0.7360\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5413 - acc: 0.7362\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5414 - acc: 0.7360\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5409 - acc: 0.7362\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5410 - acc: 0.7364\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5407 - acc: 0.7369\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5402 - acc: 0.7369\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5405 - acc: 0.7368\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5400 - acc: 0.7367\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5399 - acc: 0.7371\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5400 - acc: 0.7370\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5396 - acc: 0.7371\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 4s 145us/sample - loss: 0.5397 - acc: 0.7369s - loss: 0.5402 - acc: 0.73\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.5399 - acc: 0.7372\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5395 - acc: 0.7369\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5395 - acc: 0.7370\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5393 - acc: 0.7378\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5391 - acc: 0.7376\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5391 - acc: 0.7379\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5390 - acc: 0.7384\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5390 - acc: 0.7374\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5389 - acc: 0.7382\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5388 - acc: 0.7380\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5387 - acc: 0.7382\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5386 - acc: 0.7380\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5387 - acc: 0.7377\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5385 - acc: 0.7383\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5385 - acc: 0.7378\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5383 - acc: 0.7383\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5386 - acc: 0.7381\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5382 - acc: 0.7384\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5384 - acc: 0.7378\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5381 - acc: 0.7385\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5380 - acc: 0.7381\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5381 - acc: 0.7379\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5379 - acc: 0.7382\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5375 - acc: 0.7381\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5380 - acc: 0.7384\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5377 - acc: 0.7389\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5378 - acc: 0.7384\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5375 - acc: 0.7383\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5375 - acc: 0.7384\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5376 - acc: 0.7380\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5379 - acc: 0.7386\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5373 - acc: 0.7380\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5376 - acc: 0.7386\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5370 - acc: 0.7381\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5370 - acc: 0.7386\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5370 - acc: 0.7387\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5370 - acc: 0.7385\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5371 - acc: 0.7386\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5371 - acc: 0.7386\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5369 - acc: 0.7392\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5370 - acc: 0.7381\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5370 - acc: 0.7382\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5367 - acc: 0.7388\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5368 - acc: 0.7391\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5366 - acc: 0.7386\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5371 - acc: 0.7389\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5366 - acc: 0.7387\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5369 - acc: 0.7382\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5365 - acc: 0.7393\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5366 - acc: 0.7388\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5366 - acc: 0.7394\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5365 - acc: 0.7396\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5371 - acc: 0.7388\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5370 - acc: 0.7388\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5366 - acc: 0.7391\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5364 - acc: 0.7391\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5362 - acc: 0.7391\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5365 - acc: 0.7391\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5365 - acc: 0.7388\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5365 - acc: 0.7393\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5363 - acc: 0.7388\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5362 - acc: 0.7390\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5362 - acc: 0.7392\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5362 - acc: 0.7392\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5361 - acc: 0.7392\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5362 - acc: 0.7392\n",
      "8575/8575 - 0s - loss: 0.5567 - acc: 0.7270\n",
      "Loss: 0.5566686161127452, Accuracy: 0.7269970774650574\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "hidden_nodes_layer2 = 100\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5718 - acc: 0.7245\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5518 - acc: 0.7312\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5487 - acc: 0.7337\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5467 - acc: 0.7326\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5459 - acc: 0.7352\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5452 - acc: 0.7347\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5443 - acc: 0.7358\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5430 - acc: 0.7357\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 2s 81us/sample - loss: 0.5429 - acc: 0.7378\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5420 - acc: 0.7372\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5418 - acc: 0.7368\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5408 - acc: 0.7352\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5404 - acc: 0.7371\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5398 - acc: 0.7374\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5396 - acc: 0.7381\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5388 - acc: 0.7386\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5393 - acc: 0.7386\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5378 - acc: 0.7385\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5378 - acc: 0.7404\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5373 - acc: 0.7395\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5372 - acc: 0.7402\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5367 - acc: 0.7401\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5358 - acc: 0.7402\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5367 - acc: 0.7413\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5356 - acc: 0.7414\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5358 - acc: 0.7412\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5354 - acc: 0.7408\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5350 - acc: 0.7421\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5348 - acc: 0.7414\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5344 - acc: 0.7418\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5344 - acc: 0.7412\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5340 - acc: 0.7418\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5351 - acc: 0.7420\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5338 - acc: 0.7414\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5333 - acc: 0.7428\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5333 - acc: 0.7427\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5335 - acc: 0.7417\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5333 - acc: 0.7417\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5326 - acc: 0.7424\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5330 - acc: 0.7422\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5326 - acc: 0.7420\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5325 - acc: 0.7423\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5327 - acc: 0.7425\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5320 - acc: 0.7423\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5318 - acc: 0.7425\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5317 - acc: 0.7421\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5312 - acc: 0.7428\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5317 - acc: 0.7425\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5317 - acc: 0.7430\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5311 - acc: 0.7435\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5315 - acc: 0.7432\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5311 - acc: 0.7439\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 78us/sample - loss: 0.5314 - acc: 0.7429\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5312 - acc: 0.7429\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5311 - acc: 0.7433\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5306 - acc: 0.7429\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5311 - acc: 0.7430\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5308 - acc: 0.7440\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5306 - acc: 0.7427\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5304 - acc: 0.7433\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5303 - acc: 0.7430\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5299 - acc: 0.7437\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5303 - acc: 0.7431\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5299 - acc: 0.7437\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5309 - acc: 0.7439\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5297 - acc: 0.7428\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5298 - acc: 0.7436\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5293 - acc: 0.7436\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5297 - acc: 0.7423\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5295 - acc: 0.7438\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5294 - acc: 0.7440\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5300 - acc: 0.7434\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5294 - acc: 0.7436\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5290 - acc: 0.7442\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5304 - acc: 0.7438\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5353 - acc: 0.7431\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5287 - acc: 0.7446\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5283 - acc: 0.7439\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5285 - acc: 0.7439\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.5283 - acc: 0.7437\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5285 - acc: 0.7441\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5284 - acc: 0.7436\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5358 - acc: 0.7438\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5288 - acc: 0.7435\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5279 - acc: 0.7437\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5283 - acc: 0.7440\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5288 - acc: 0.7436\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5285 - acc: 0.7442\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5283 - acc: 0.7442\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5284 - acc: 0.7438\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5281 - acc: 0.7437\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5279 - acc: 0.7439\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5281 - acc: 0.7441\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5282 - acc: 0.7441\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5280 - acc: 0.7439\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5286 - acc: 0.7441\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5288 - acc: 0.7438\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5283 - acc: 0.7442\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5319 - acc: 0.7439\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5352 - acc: 0.7429\n",
      "8575/8575 - 0s - loss: 0.6940 - acc: 0.7261\n",
      "Loss: 0.6940009418798953, Accuracy: 0.726064145565033\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is even worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to check if the model is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5407 - acc: 0.7437\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5269 - acc: 0.7439\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5271 - acc: 0.7442\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5273 - acc: 0.7440\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5275 - acc: 0.7444\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5273 - acc: 0.7439\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5280 - acc: 0.7444\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5272 - acc: 0.7443\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5275 - acc: 0.7442\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5275 - acc: 0.7438\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5289 - acc: 0.7445\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5281 - acc: 0.7439\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5270 - acc: 0.7445\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5269 - acc: 0.7444\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5280 - acc: 0.7442\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5274 - acc: 0.7443\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5322 - acc: 0.7442\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5550 - acc: 0.7443\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5277 - acc: 0.7447\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5279 - acc: 0.7443\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5265 - acc: 0.7447\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5264 - acc: 0.7442\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5266 - acc: 0.7444\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5267 - acc: 0.7444\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5269 - acc: 0.7446\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5267 - acc: 0.7438\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5279 - acc: 0.7443\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5802 - acc: 0.7440\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5460 - acc: 0.7432\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5261 - acc: 0.7446\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5257 - acc: 0.7440\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5260 - acc: 0.7443\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5270 - acc: 0.7445\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5260 - acc: 0.7439\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5263 - acc: 0.7442\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5266 - acc: 0.7441\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5263 - acc: 0.7446\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5266 - acc: 0.7443\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5275 - acc: 0.7442\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5265 - acc: 0.7446\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5263 - acc: 0.7447\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5266 - acc: 0.7446\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5266 - acc: 0.7447\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5273 - acc: 0.7446\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5266 - acc: 0.7442\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5269 - acc: 0.7447\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5265 - acc: 0.7453\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5261 - acc: 0.7450\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5275 - acc: 0.7443\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5262 - acc: 0.7446\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5263 - acc: 0.7442\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5263 - acc: 0.7449\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5274 - acc: 0.7446\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5263 - acc: 0.7450\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5262 - acc: 0.7438\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5260 - acc: 0.7444\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5272 - acc: 0.7445\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5261 - acc: 0.7448\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5259 - acc: 0.7445\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5266 - acc: 0.7448\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5259 - acc: 0.7442\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5358 - acc: 0.7443\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5256 - acc: 0.7448\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5253 - acc: 0.7444\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5265 - acc: 0.7444\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5258 - acc: 0.7442\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5255 - acc: 0.7450\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5261 - acc: 0.7446\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5260 - acc: 0.7444\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5258 - acc: 0.7444\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5258 - acc: 0.7449\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5264 - acc: 0.7452\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5256 - acc: 0.7446\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5255 - acc: 0.7446\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5262 - acc: 0.7448\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5270 - acc: 0.7447\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5254 - acc: 0.7454\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5256 - acc: 0.7446\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5946 - acc: 0.7444\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5318 - acc: 0.7452\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5248 - acc: 0.7446\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5248 - acc: 0.7446\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5249 - acc: 0.7448\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5248 - acc: 0.7451\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5252 - acc: 0.7448\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5250 - acc: 0.7450\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5250 - acc: 0.7449\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5259 - acc: 0.7448\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5254 - acc: 0.7453\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5252 - acc: 0.7448\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5261 - acc: 0.7451\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5250 - acc: 0.7449\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5256 - acc: 0.7447\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5263 - acc: 0.7441\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5248 - acc: 0.7451\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5254 - acc: 0.7455\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5255 - acc: 0.7446\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5250 - acc: 0.7449\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5258 - acc: 0.7448\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5263 - acc: 0.7444\n",
      "25724/25724 - 1s - loss: 0.5224 - acc: 0.7455\n",
      "Loss: 0.5224447522855179, Accuracy: 0.7454516887664795\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_train_scaled,y_train,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't get trained properly. Will add one more layer of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "hidden_nodes_layer2 = 100\n",
    "hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5709 - acc: 0.7240\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5525 - acc: 0.7316\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5485 - acc: 0.7333\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5466 - acc: 0.7353\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5461 - acc: 0.7346\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5447 - acc: 0.7360\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5432 - acc: 0.7374\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5426 - acc: 0.7372\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5413 - acc: 0.7373\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5409 - acc: 0.7376\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5403 - acc: 0.7387\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5397 - acc: 0.7388\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5398 - acc: 0.7393\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5389 - acc: 0.7391\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5380 - acc: 0.7398\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5382 - acc: 0.7402\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5374 - acc: 0.7401\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5371 - acc: 0.7400\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5369 - acc: 0.7400\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 80us/sample - loss: 0.5361 - acc: 0.7404\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5359 - acc: 0.7411\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5360 - acc: 0.7405\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.5351 - acc: 0.7409\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 78us/sample - loss: 0.5346 - acc: 0.7409\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.5342 - acc: 0.7411\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 82us/sample - loss: 0.5346 - acc: 0.7407\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5347 - acc: 0.7408\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5343 - acc: 0.7416\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5334 - acc: 0.7417\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5334 - acc: 0.7414\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5327 - acc: 0.7424\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5330 - acc: 0.7417\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5325 - acc: 0.7419\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5326 - acc: 0.7421\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5322 - acc: 0.7421\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5319 - acc: 0.7423\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5320 - acc: 0.7421\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5318 - acc: 0.7423\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5314 - acc: 0.7426\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5317 - acc: 0.7427\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5312 - acc: 0.7418\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5310 - acc: 0.7425\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5313 - acc: 0.7431\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5306 - acc: 0.7422\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5304 - acc: 0.7435\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5303 - acc: 0.7430\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5301 - acc: 0.7439\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5300 - acc: 0.7430\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5302 - acc: 0.7431\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5299 - acc: 0.7437\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5295 - acc: 0.7435\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5305 - acc: 0.7435\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5295 - acc: 0.7433\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5294 - acc: 0.7437\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5287 - acc: 0.7436\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5290 - acc: 0.7432\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5285 - acc: 0.7437\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5286 - acc: 0.7437\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5288 - acc: 0.7442\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5286 - acc: 0.7436\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5285 - acc: 0.7441\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5286 - acc: 0.7434\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5287 - acc: 0.7437\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5284 - acc: 0.7434\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5283 - acc: 0.7436\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5277 - acc: 0.7437\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5279 - acc: 0.7439\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5278 - acc: 0.7444\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5291 - acc: 0.7447\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5278 - acc: 0.7439\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5275 - acc: 0.7439\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5276 - acc: 0.7446\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5274 - acc: 0.7447\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5290 - acc: 0.7437\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5331 - acc: 0.7441\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5286 - acc: 0.7437\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5271 - acc: 0.7442\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5269 - acc: 0.7443\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5272 - acc: 0.7443\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5272 - acc: 0.7444\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5272 - acc: 0.7442\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5272 - acc: 0.7441\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5271 - acc: 0.7445\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5274 - acc: 0.7440\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5313 - acc: 0.7439\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5320 - acc: 0.7446\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5270 - acc: 0.7446\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5261 - acc: 0.7443\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5261 - acc: 0.7450\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5263 - acc: 0.7442\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5269 - acc: 0.7440\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5270 - acc: 0.7441\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5268 - acc: 0.7439\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5286 - acc: 0.7438\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5265 - acc: 0.7446\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5260 - acc: 0.7447\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5262 - acc: 0.7445\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5267 - acc: 0.7441\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5266 - acc: 0.7440\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5267 - acc: 0.7446\n",
      "8575/8575 - 0s - loss: 0.5930 - acc: 0.7275\n",
      "Loss: 0.5929508913779745, Accuracy: 0.7274635434150696\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case, want to try three hidden layers with tanh as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "hidden_nodes_layer2 = 100\n",
    "hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"tanh\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"tanh\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5661 - acc: 0.7244\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5534 - acc: 0.7289\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5495 - acc: 0.7309\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5468 - acc: 0.7332\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5451 - acc: 0.7337\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5441 - acc: 0.7342\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5428 - acc: 0.7365\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5417 - acc: 0.7361\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5407 - acc: 0.7382\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5399 - acc: 0.7367\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.5384 - acc: 0.7368\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5379 - acc: 0.7367\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5374 - acc: 0.7400\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5368 - acc: 0.7388\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5361 - acc: 0.7385\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5354 - acc: 0.7397\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5353 - acc: 0.7397\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5346 - acc: 0.7404\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5343 - acc: 0.7415\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5338 - acc: 0.7395\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5335 - acc: 0.7400\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5334 - acc: 0.7410\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.5324 - acc: 0.7418\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5326 - acc: 0.7419\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5316 - acc: 0.7421\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5323 - acc: 0.7413\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5320 - acc: 0.7417\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5308 - acc: 0.7417\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5307 - acc: 0.7423\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5305 - acc: 0.7422\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5304 - acc: 0.7426\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5304 - acc: 0.7427\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5303 - acc: 0.7430\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.5301 - acc: 0.7428\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.5298 - acc: 0.7426\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5297 - acc: 0.7432\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5290 - acc: 0.7439\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5293 - acc: 0.7429\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5290 - acc: 0.7429\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5291 - acc: 0.7431\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5290 - acc: 0.7431\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5288 - acc: 0.7431\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5288 - acc: 0.7432\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5279 - acc: 0.7439\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.5282 - acc: 0.7439\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 2s 81us/sample - loss: 0.5283 - acc: 0.7428\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5281 - acc: 0.7435\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5289 - acc: 0.7431\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5285 - acc: 0.7428\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5274 - acc: 0.7438\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5275 - acc: 0.7440\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5274 - acc: 0.7440\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5278 - acc: 0.7435\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5273 - acc: 0.7442\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5269 - acc: 0.7451\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5271 - acc: 0.7441\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5279 - acc: 0.7434\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5278 - acc: 0.7439\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5273 - acc: 0.7439\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5269 - acc: 0.7439\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5272 - acc: 0.7448\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5275 - acc: 0.7439\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5269 - acc: 0.7443\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5266 - acc: 0.7446\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5263 - acc: 0.7440\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5266 - acc: 0.7431\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5268 - acc: 0.7439\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5266 - acc: 0.7444\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5264 - acc: 0.7440\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5266 - acc: 0.7434\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5263 - acc: 0.7445\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5264 - acc: 0.7429\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5261 - acc: 0.7455\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5267 - acc: 0.7452\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5267 - acc: 0.7444\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5264 - acc: 0.7441\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5258 - acc: 0.7441\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5260 - acc: 0.7449\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5268 - acc: 0.7446\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5264 - acc: 0.7450\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5263 - acc: 0.7442\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5262 - acc: 0.7453\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5264 - acc: 0.7440\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5264 - acc: 0.7442\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5262 - acc: 0.7441\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5261 - acc: 0.7441\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5259 - acc: 0.7446\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.5260 - acc: 0.7451\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5254 - acc: 0.7444\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5260 - acc: 0.7451\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5259 - acc: 0.7447\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5261 - acc: 0.7446\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5259 - acc: 0.7448\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5258 - acc: 0.7444\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5253 - acc: 0.7452\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5259 - acc: 0.7449\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5259 - acc: 0.7451\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5261 - acc: 0.7444\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5255 - acc: 0.7446\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5255 - acc: 0.7456\n",
      "8575/8575 - 0s - loss: 0.5563 - acc: 0.7276\n",
      "Loss: 0.5563185447854134, Accuracy: 0.727580189704895\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is still high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying other options - linear activation function in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "hidden_nodes_layer2 = 100\n",
    "# hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"tanh\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.2658 - acc: 0.7086\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.2344 - acc: 0.7228\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1914 - acc: 0.7297\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1853 - acc: 0.7334\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1846 - acc: 0.7331\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1842 - acc: 0.7339\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1842 - acc: 0.7355\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1840 - acc: 0.7339\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1843 - acc: 0.7337\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1841 - acc: 0.7357\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1836 - acc: 0.7366\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1839 - acc: 0.7355\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.1833 - acc: 0.7346\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1828 - acc: 0.7361\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1827 - acc: 0.7362\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1824 - acc: 0.7352\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1823 - acc: 0.7361\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1822 - acc: 0.7369\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1823 - acc: 0.7360\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1821 - acc: 0.7370\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1819 - acc: 0.7387\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1815 - acc: 0.7383\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1813 - acc: 0.7378\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1816 - acc: 0.7387\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1813 - acc: 0.7381\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1811 - acc: 0.7385\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1812 - acc: 0.7392\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1809 - acc: 0.7394\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1808 - acc: 0.7394\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1807 - acc: 0.7405\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1806 - acc: 0.7392\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1810 - acc: 0.7405\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1804 - acc: 0.7394\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1803 - acc: 0.7406\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1803 - acc: 0.7403\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1802 - acc: 0.7410\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1801 - acc: 0.7406\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1802 - acc: 0.7402\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1802 - acc: 0.7416\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1800 - acc: 0.7409\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1833 - acc: 0.7399\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1800 - acc: 0.7403\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1800 - acc: 0.7402\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1799 - acc: 0.7403\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1800 - acc: 0.7413\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1795 - acc: 0.7417\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1794 - acc: 0.7408\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1797 - acc: 0.7413\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1796 - acc: 0.7414\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1797 - acc: 0.7406\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1792 - acc: 0.7418\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1796 - acc: 0.7400\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1796 - acc: 0.7418\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1796 - acc: 0.7415\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1794 - acc: 0.7419\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1794 - acc: 0.7408\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.1798 - acc: 0.7416\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1793 - acc: 0.7419\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1792 - acc: 0.7418\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1793 - acc: 0.7424\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1792 - acc: 0.7416\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1793 - acc: 0.7409\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1791 - acc: 0.7431\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1793 - acc: 0.7419\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1788 - acc: 0.7432\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1797 - acc: 0.7421\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1790 - acc: 0.7417\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1789 - acc: 0.7426\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1788 - acc: 0.7430\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1790 - acc: 0.7421\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1789 - acc: 0.7426\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1790 - acc: 0.7431\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1785 - acc: 0.7422\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1791 - acc: 0.7426\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1793 - acc: 0.7423\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1788 - acc: 0.7432\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1788 - acc: 0.7419\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1788 - acc: 0.7434\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1785 - acc: 0.7430\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1786 - acc: 0.7427\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1785 - acc: 0.7432\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1785 - acc: 0.7431\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1785 - acc: 0.7432\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1786 - acc: 0.7428\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1787 - acc: 0.7435\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1785 - acc: 0.7435\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1786 - acc: 0.7421\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1785 - acc: 0.7433\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1784 - acc: 0.7438\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1785 - acc: 0.7431\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1784 - acc: 0.7430\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1782 - acc: 0.7434\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1787 - acc: 0.7436\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1782 - acc: 0.7429\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1793 - acc: 0.7428\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1784 - acc: 0.7427\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.1783 - acc: 0.7428\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1783 - acc: 0.7425\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1781 - acc: 0.7437\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1780 - acc: 0.7433\n",
      "8575/8575 - 0s - loss: 0.1873 - acc: 0.7264\n",
      "Loss: 0.18725520545817673, Accuracy: 0.7264139652252197\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x221fb06f148>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x221fb83fdc8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_df.plot(y=\"acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see this is the best result so far - same accuracy but much lower loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "hidden_nodes_layer2 = 100\n",
    "hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.2160 - acc: 0.7129\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1960 - acc: 0.7243\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1883 - acc: 0.7281\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1859 - acc: 0.7308\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1848 - acc: 0.7317\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1847 - acc: 0.7324\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1843 - acc: 0.7345\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1840 - acc: 0.7351\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.1839 - acc: 0.7369\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1840 - acc: 0.7339\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1832 - acc: 0.7339\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1831 - acc: 0.7368\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1829 - acc: 0.7362\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1828 - acc: 0.7366\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.1823 - acc: 0.7381\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 3s 114us/sample - loss: 0.1820 - acc: 0.7366\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1818 - acc: 0.7372\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1818 - acc: 0.7371\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1817 - acc: 0.7384\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 79us/sample - loss: 0.1813 - acc: 0.7380\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 80us/sample - loss: 0.1813 - acc: 0.7386\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1809 - acc: 0.7387\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1810 - acc: 0.7391\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1808 - acc: 0.7390\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1809 - acc: 0.7393\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.1803 - acc: 0.7392\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.1803 - acc: 0.7400\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1804 - acc: 0.7396\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.1800 - acc: 0.7407\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1798 - acc: 0.7406\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.1799 - acc: 0.7405\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1798 - acc: 0.7409\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.1794 - acc: 0.7418\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1795 - acc: 0.7412\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1795 - acc: 0.7413\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1794 - acc: 0.7415\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1795 - acc: 0.7416\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1792 - acc: 0.7419\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1793 - acc: 0.7420\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1790 - acc: 0.7421\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1791 - acc: 0.7420\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1788 - acc: 0.7420\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1788 - acc: 0.7420\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.1789 - acc: 0.7418\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1789 - acc: 0.7409\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1791 - acc: 0.7419\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.1786 - acc: 0.7425\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 2s 78us/sample - loss: 0.1785 - acc: 0.7419\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1785 - acc: 0.7417\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 79us/sample - loss: 0.1784 - acc: 0.7422\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1784 - acc: 0.7425\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1786 - acc: 0.7424\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1783 - acc: 0.7431\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1781 - acc: 0.7428\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 2s 79us/sample - loss: 0.1781 - acc: 0.7420\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1783 - acc: 0.7422\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1781 - acc: 0.7423\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1780 - acc: 0.7430\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1780 - acc: 0.7434\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.1781 - acc: 0.7425\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1780 - acc: 0.7431\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1780 - acc: 0.7424\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1779 - acc: 0.7430\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1780 - acc: 0.7435\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1778 - acc: 0.7432\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 2s 80us/sample - loss: 0.1777 - acc: 0.7430\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1781 - acc: 0.7437\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1778 - acc: 0.7428\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1785 - acc: 0.7436\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1777 - acc: 0.7434\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1776 - acc: 0.7440\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1779 - acc: 0.7438\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1775 - acc: 0.7440\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1774 - acc: 0.7434\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.1775 - acc: 0.7441\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1775 - acc: 0.7441\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1773 - acc: 0.7438\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1777 - acc: 0.7433\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.1774 - acc: 0.7433\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1774 - acc: 0.7435\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1805 - acc: 0.7437\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1776 - acc: 0.7441\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - ETA: 0s - loss: 0.1771 - acc: 0.744 - 2s 75us/sample - loss: 0.1774 - acc: 0.7441\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 80us/sample - loss: 0.1771 - acc: 0.7435\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1774 - acc: 0.7442\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.1773 - acc: 0.7449\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1772 - acc: 0.7436\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.1771 - acc: 0.7435\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 78us/sample - loss: 0.1772 - acc: 0.7434\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1773 - acc: 0.7434\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1774 - acc: 0.7425\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1772 - acc: 0.7439\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.1771 - acc: 0.7437\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.1773 - acc: 0.7441\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.1771 - acc: 0.7434\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.1770 - acc: 0.7442\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.1773 - acc: 0.7434\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.1813 - acc: 0.7435\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.2082 - acc: 0.7441\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.1768 - acc: 0.7444\n",
      "8575/8575 - 0s - loss: 0.1863 - acc: 0.7264\n",
      "Loss: 0.186254895795191, Accuracy: 0.7264139652252197\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference. So there is no point to add more layers. But the linear activation function in the output layer works better. What if we add more neurons to the second layer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "hidden_nodes_layer2 = 200\n",
    "hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 92us/sample - loss: 0.2186 - acc: 0.7127\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1965 - acc: 0.7254\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1925 - acc: 0.7298\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1866 - acc: 0.7308\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1849 - acc: 0.7329\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1842 - acc: 0.7327\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1844 - acc: 0.7339\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1842 - acc: 0.7343\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1840 - acc: 0.7347\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.1837 - acc: 0.7340\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1842 - acc: 0.7348\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1833 - acc: 0.7349\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1826 - acc: 0.7365\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1833 - acc: 0.7367\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.1825 - acc: 0.7377\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1817 - acc: 0.7378\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1819 - acc: 0.7379\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1815 - acc: 0.7376\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1813 - acc: 0.7387\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1810 - acc: 0.7388\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1811 - acc: 0.7382\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1811 - acc: 0.7383\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1808 - acc: 0.7390\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1806 - acc: 0.7399\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1807 - acc: 0.7404\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1804 - acc: 0.7395\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1803 - acc: 0.7399\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1804 - acc: 0.7405\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 3s 97us/sample - loss: 0.1805 - acc: 0.7393\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1801 - acc: 0.7399\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1798 - acc: 0.7407\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1796 - acc: 0.7410\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1797 - acc: 0.7401\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1795 - acc: 0.7419\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1794 - acc: 0.7421\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1793 - acc: 0.7411\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 2s 95us/sample - loss: 0.1794 - acc: 0.7413\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1793 - acc: 0.7427\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 2s 94us/sample - loss: 0.1790 - acc: 0.7426\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1788 - acc: 0.7424\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1799 - acc: 0.7415\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1789 - acc: 0.7413\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1787 - acc: 0.7423\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1788 - acc: 0.7421\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1786 - acc: 0.7411\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1787 - acc: 0.7428\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.1784 - acc: 0.7426\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1787 - acc: 0.7418\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1786 - acc: 0.7432\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1782 - acc: 0.7428\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1785 - acc: 0.7423\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1785 - acc: 0.7426\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1783 - acc: 0.7435\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1781 - acc: 0.7428\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1779 - acc: 0.7432\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1782 - acc: 0.7425\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1781 - acc: 0.7430\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 2s 94us/sample - loss: 0.1782 - acc: 0.7432\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1780 - acc: 0.7432\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1777 - acc: 0.7432\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1778 - acc: 0.7432\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 2s 95us/sample - loss: 0.1778 - acc: 0.7437\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1777 - acc: 0.7435\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1780 - acc: 0.7436\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1777 - acc: 0.7432\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1776 - acc: 0.7428\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1775 - acc: 0.7438\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1777 - acc: 0.7433\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1777 - acc: 0.7436\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1776 - acc: 0.7436\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1776 - acc: 0.7435\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1777 - acc: 0.7435\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1774 - acc: 0.7437\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1782 - acc: 0.7434\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1774 - acc: 0.7433\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1773 - acc: 0.7439\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1775 - acc: 0.7435\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1773 - acc: 0.7437\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1774 - acc: 0.7440\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.1774 - acc: 0.7437\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1774 - acc: 0.7434\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1774 - acc: 0.7441\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1772 - acc: 0.7439\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1775 - acc: 0.7436\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - ETA: 0s - loss: 0.1773 - acc: 0.743 - 2s 86us/sample - loss: 0.1773 - acc: 0.7439\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1770 - acc: 0.7437\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1771 - acc: 0.7441\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1769 - acc: 0.7446\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.1772 - acc: 0.7443\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 96us/sample - loss: 0.1772 - acc: 0.7442\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 3s 110us/sample - loss: 0.1774 - acc: 0.7434\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 92us/sample - loss: 0.1772 - acc: 0.7440\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1772 - acc: 0.7434\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1768 - acc: 0.7442\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 92us/sample - loss: 0.1769 - acc: 0.7442\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1771 - acc: 0.7447\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1770 - acc: 0.7444\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1771 - acc: 0.7433\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1772 - acc: 0.7437\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1771 - acc: 0.7443\n",
      "8575/8575 - 0s - loss: 0.1878 - acc: 0.7268\n",
      "Loss: 0.1877756436764325, Accuracy: 0.7267638444900513\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will add more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 232\n",
    "hidden_nodes_layer2 = 200\n",
    "hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.2250 - acc: 0.7144\n",
      "Epoch 2/200\n",
      "25724/25724 [==============================] - 3s 99us/sample - loss: 0.1942 - acc: 0.7262\n",
      "Epoch 3/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1892 - acc: 0.7292\n",
      "Epoch 4/200\n",
      "25724/25724 [==============================] - 2s 92us/sample - loss: 0.1909 - acc: 0.7302\n",
      "Epoch 5/200\n",
      "25724/25724 [==============================] - 2s 92us/sample - loss: 0.1926 - acc: 0.7295\n",
      "Epoch 6/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1864 - acc: 0.7326\n",
      "Epoch 7/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1849 - acc: 0.7344\n",
      "Epoch 8/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1846 - acc: 0.7325\n",
      "Epoch 9/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1841 - acc: 0.7344\n",
      "Epoch 10/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1838 - acc: 0.7350\n",
      "Epoch 11/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1836 - acc: 0.7345\n",
      "Epoch 12/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1833 - acc: 0.7352\n",
      "Epoch 13/200\n",
      "25724/25724 [==============================] - 2s 95us/sample - loss: 0.1828 - acc: 0.7357\n",
      "Epoch 14/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1828 - acc: 0.7359\n",
      "Epoch 15/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1824 - acc: 0.7376\n",
      "Epoch 16/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1823 - acc: 0.7375\n",
      "Epoch 17/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1825 - acc: 0.7371\n",
      "Epoch 18/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1818 - acc: 0.7375\n",
      "Epoch 19/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1818 - acc: 0.7378\n",
      "Epoch 20/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1817 - acc: 0.7381\n",
      "Epoch 21/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1815 - acc: 0.7389\n",
      "Epoch 22/200\n",
      "25724/25724 [==============================] - 2s 94us/sample - loss: 0.1810 - acc: 0.7394\n",
      "Epoch 23/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1810 - acc: 0.7391\n",
      "Epoch 24/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1809 - acc: 0.7383\n",
      "Epoch 25/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1815 - acc: 0.7389\n",
      "Epoch 26/200\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1806 - acc: 0.7383\n",
      "Epoch 27/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1805 - acc: 0.7407\n",
      "Epoch 28/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1804 - acc: 0.7410\n",
      "Epoch 29/200\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1801 - acc: 0.7400\n",
      "Epoch 30/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1799 - acc: 0.7407\n",
      "Epoch 31/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1800 - acc: 0.7405\n",
      "Epoch 32/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1797 - acc: 0.7403\n",
      "Epoch 33/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1795 - acc: 0.7408\n",
      "Epoch 34/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1798 - acc: 0.7402\n",
      "Epoch 35/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1796 - acc: 0.7399\n",
      "Epoch 36/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1793 - acc: 0.7420\n",
      "Epoch 37/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1794 - acc: 0.7419\n",
      "Epoch 38/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1793 - acc: 0.7417\n",
      "Epoch 39/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1789 - acc: 0.7425\n",
      "Epoch 40/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1794 - acc: 0.7418\n",
      "Epoch 41/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1789 - acc: 0.7425\n",
      "Epoch 42/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1789 - acc: 0.7420\n",
      "Epoch 43/200\n",
      "25724/25724 [==============================] - 2s 94us/sample - loss: 0.1785 - acc: 0.7422\n",
      "Epoch 44/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1790 - acc: 0.7421\n",
      "Epoch 45/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1785 - acc: 0.7423\n",
      "Epoch 46/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1786 - acc: 0.7435\n",
      "Epoch 47/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1783 - acc: 0.7425\n",
      "Epoch 48/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1783 - acc: 0.7423\n",
      "Epoch 49/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1784 - acc: 0.7431\n",
      "Epoch 50/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1785 - acc: 0.7426\n",
      "Epoch 51/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1783 - acc: 0.7425\n",
      "Epoch 52/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1780 - acc: 0.7436\n",
      "Epoch 53/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1806 - acc: 0.7409\n",
      "Epoch 54/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1785 - acc: 0.7423\n",
      "Epoch 55/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1781 - acc: 0.7430\n",
      "Epoch 56/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1780 - acc: 0.7432\n",
      "Epoch 57/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1779 - acc: 0.7437\n",
      "Epoch 58/200\n",
      "25724/25724 [==============================] - 2s 92us/sample - loss: 0.1781 - acc: 0.7434\n",
      "Epoch 59/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1780 - acc: 0.7430\n",
      "Epoch 60/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1781 - acc: 0.7442\n",
      "Epoch 61/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1781 - acc: 0.7431\n",
      "Epoch 62/200\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1778 - acc: 0.7432\n",
      "Epoch 63/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1778 - acc: 0.7428\n",
      "Epoch 64/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1777 - acc: 0.7430\n",
      "Epoch 65/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1779 - acc: 0.7435\n",
      "Epoch 66/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1778 - acc: 0.7432\n",
      "Epoch 67/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1778 - acc: 0.7437\n",
      "Epoch 68/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1777 - acc: 0.7439\n",
      "Epoch 69/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1776 - acc: 0.7443\n",
      "Epoch 70/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1778 - acc: 0.7433\n",
      "Epoch 71/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1775 - acc: 0.7434\n",
      "Epoch 72/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1774 - acc: 0.7440\n",
      "Epoch 73/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1776 - acc: 0.7447\n",
      "Epoch 74/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1777 - acc: 0.7437\n",
      "Epoch 75/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1771 - acc: 0.7446\n",
      "Epoch 76/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1775 - acc: 0.7442\n",
      "Epoch 77/200\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1776 - acc: 0.7433\n",
      "Epoch 78/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1773 - acc: 0.7439\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1771 - acc: 0.7442\n",
      "Epoch 80/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1772 - acc: 0.7439\n",
      "Epoch 81/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1772 - acc: 0.7441\n",
      "Epoch 82/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1772 - acc: 0.7441\n",
      "Epoch 83/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1776 - acc: 0.7433\n",
      "Epoch 84/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1776 - acc: 0.7434\n",
      "Epoch 85/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1777 - acc: 0.7432\n",
      "Epoch 86/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1773 - acc: 0.7442\n",
      "Epoch 87/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1774 - acc: 0.7433\n",
      "Epoch 88/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1771 - acc: 0.7437\n",
      "Epoch 89/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1772 - acc: 0.7437\n",
      "Epoch 90/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1772 - acc: 0.7441\n",
      "Epoch 91/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1769 - acc: 0.7446\n",
      "Epoch 92/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1771 - acc: 0.7441\n",
      "Epoch 93/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1771 - acc: 0.7439\n",
      "Epoch 94/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1771 - acc: 0.7442\n",
      "Epoch 95/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1784 - acc: 0.7440\n",
      "Epoch 96/200\n",
      "25724/25724 [==============================] - 2s 94us/sample - loss: 0.1772 - acc: 0.7441\n",
      "Epoch 97/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1772 - acc: 0.7443\n",
      "Epoch 98/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1769 - acc: 0.7448\n",
      "Epoch 99/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1790 - acc: 0.7444\n",
      "Epoch 100/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1773 - acc: 0.7445\n",
      "Epoch 101/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1771 - acc: 0.7448\n",
      "Epoch 102/200\n",
      "25724/25724 [==============================] - 2s 97us/sample - loss: 0.1767 - acc: 0.7448\n",
      "Epoch 103/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1769 - acc: 0.7433\n",
      "Epoch 104/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1768 - acc: 0.7444\n",
      "Epoch 105/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1770 - acc: 0.7444\n",
      "Epoch 106/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1768 - acc: 0.7439\n",
      "Epoch 107/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1774 - acc: 0.7448\n",
      "Epoch 108/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1769 - acc: 0.7447\n",
      "Epoch 109/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1769 - acc: 0.7442\n",
      "Epoch 110/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1886 - acc: 0.7445\n",
      "Epoch 111/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1772 - acc: 0.7446\n",
      "Epoch 112/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1782 - acc: 0.7439\n",
      "Epoch 113/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1773 - acc: 0.7447\n",
      "Epoch 114/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1768 - acc: 0.7448\n",
      "Epoch 115/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1769 - acc: 0.7447\n",
      "Epoch 116/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1769 - acc: 0.7437\n",
      "Epoch 117/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1769 - acc: 0.7440\n",
      "Epoch 118/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1769 - acc: 0.7443\n",
      "Epoch 119/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1767 - acc: 0.7448\n",
      "Epoch 120/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1769 - acc: 0.7448\n",
      "Epoch 121/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1797 - acc: 0.7442\n",
      "Epoch 122/200\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1772 - acc: 0.7444\n",
      "Epoch 123/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1775 - acc: 0.7442\n",
      "Epoch 124/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1769 - acc: 0.7444\n",
      "Epoch 125/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1765 - acc: 0.7451\n",
      "Epoch 126/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1766 - acc: 0.7446\n",
      "Epoch 127/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1767 - acc: 0.7443\n",
      "Epoch 128/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1766 - acc: 0.7452\n",
      "Epoch 129/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1767 - acc: 0.7448\n",
      "Epoch 130/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1766 - acc: 0.7445\n",
      "Epoch 131/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1766 - acc: 0.7441\n",
      "Epoch 132/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1765 - acc: 0.7451\n",
      "Epoch 133/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1766 - acc: 0.7449\n",
      "Epoch 134/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1767 - acc: 0.7441\n",
      "Epoch 135/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1766 - acc: 0.7448\n",
      "Epoch 136/200\n",
      "25724/25724 [==============================] - 2s 97us/sample - loss: 0.1765 - acc: 0.7440\n",
      "Epoch 137/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1767 - acc: 0.7448\n",
      "Epoch 138/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1774 - acc: 0.7446\n",
      "Epoch 139/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1766 - acc: 0.7442\n",
      "Epoch 140/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1766 - acc: 0.7438\n",
      "Epoch 141/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1767 - acc: 0.7449\n",
      "Epoch 142/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1766 - acc: 0.7447\n",
      "Epoch 143/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1765 - acc: 0.7444\n",
      "Epoch 144/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1766 - acc: 0.7453\n",
      "Epoch 145/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1765 - acc: 0.7443\n",
      "Epoch 146/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1768 - acc: 0.7451\n",
      "Epoch 147/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1765 - acc: 0.7446\n",
      "Epoch 148/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1764 - acc: 0.7449\n",
      "Epoch 149/200\n",
      "25724/25724 [==============================] - 2s 94us/sample - loss: 0.1765 - acc: 0.7450\n",
      "Epoch 150/200\n",
      "25724/25724 [==============================] - 2s 96us/sample - loss: 0.1766 - acc: 0.7444\n",
      "Epoch 151/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1887 - acc: 0.7455\n",
      "Epoch 152/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1771 - acc: 0.7452\n",
      "Epoch 153/200\n",
      "25724/25724 [==============================] - 2s 91us/sample - loss: 0.1766 - acc: 0.7451\n",
      "Epoch 154/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1766 - acc: 0.7446\n",
      "Epoch 155/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1766 - acc: 0.7433\n",
      "Epoch 156/200\n",
      "25724/25724 [==============================] - 2s 95us/sample - loss: 0.1776 - acc: 0.7444\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1765 - acc: 0.7446\n",
      "Epoch 158/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1766 - acc: 0.7447\n",
      "Epoch 159/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1764 - acc: 0.7448\n",
      "Epoch 160/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1765 - acc: 0.7452\n",
      "Epoch 161/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1765 - acc: 0.7448\n",
      "Epoch 162/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1764 - acc: 0.7444\n",
      "Epoch 163/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1764 - acc: 0.7438\n",
      "Epoch 164/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1800 - acc: 0.7443\n",
      "Epoch 165/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1765 - acc: 0.7448\n",
      "Epoch 166/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1765 - acc: 0.7455\n",
      "Epoch 167/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1763 - acc: 0.7446\n",
      "Epoch 168/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1764 - acc: 0.7446\n",
      "Epoch 169/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1763 - acc: 0.7452\n",
      "Epoch 170/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1768 - acc: 0.7450\n",
      "Epoch 171/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1766 - acc: 0.7442\n",
      "Epoch 172/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1764 - acc: 0.7454\n",
      "Epoch 173/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1763 - acc: 0.7449\n",
      "Epoch 174/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1764 - acc: 0.7443\n",
      "Epoch 175/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1764 - acc: 0.7451\n",
      "Epoch 176/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1763 - acc: 0.7449\n",
      "Epoch 177/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.1764 - acc: 0.7456\n",
      "Epoch 178/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1763 - acc: 0.7451\n",
      "Epoch 179/200\n",
      "25724/25724 [==============================] - 2s 85us/sample - loss: 0.1765 - acc: 0.7449\n",
      "Epoch 180/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1764 - acc: 0.7448\n",
      "Epoch 181/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1764 - acc: 0.7450\n",
      "Epoch 182/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1764 - acc: 0.7449\n",
      "Epoch 183/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1763 - acc: 0.7447\n",
      "Epoch 184/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1764 - acc: 0.7452\n",
      "Epoch 185/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1763 - acc: 0.7449\n",
      "Epoch 186/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1765 - acc: 0.7452\n",
      "Epoch 187/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1763 - acc: 0.7451\n",
      "Epoch 188/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1762 - acc: 0.7445\n",
      "Epoch 189/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1765 - acc: 0.7450\n",
      "Epoch 190/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1762 - acc: 0.7452\n",
      "Epoch 191/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.1762 - acc: 0.7449\n",
      "Epoch 192/200\n",
      "25724/25724 [==============================] - 2s 89us/sample - loss: 0.1764 - acc: 0.7439\n",
      "Epoch 193/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1821 - acc: 0.7446\n",
      "Epoch 194/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1766 - acc: 0.7450\n",
      "Epoch 195/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1761 - acc: 0.7452\n",
      "Epoch 196/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1762 - acc: 0.7447\n",
      "Epoch 197/200\n",
      "25724/25724 [==============================] - 2s 87us/sample - loss: 0.1762 - acc: 0.7448\n",
      "Epoch 198/200\n",
      "25724/25724 [==============================] - 2s 94us/sample - loss: 0.1763 - acc: 0.7453\n",
      "Epoch 199/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.1762 - acc: 0.7457\n",
      "Epoch 200/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.1761 - acc: 0.7449\n",
      "8575/8575 - 0s - loss: 0.1866 - acc: 0.7273\n",
      "Loss: 0.18661614845514993, Accuracy: 0.7273469567298889\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 200 epochs\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=200) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x221fe7ff208>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x221fe86b888>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_df.plot(y=\"acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: This data set may not be perfect for funding success prediction. The max result that we could get is 74.47% accuracy and 17.61% loss. \n",
    "### In the file \"Extension_challenge_experiment.ipynb\" I was doing an analysis with reduced features based on my own judgment - I droped \"Classification\" column. The goal was to test how it impacts the training. The thought process behind is that this data attribute may not be relevant to the applicant status. This feature is more like an identification because this is a goverment organization classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of curiosity I want to apply PCA and see if reduced dimension will make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2764198.68206174],\n",
       "       [-2660608.6820607 ],\n",
       "       [-2764198.68206071],\n",
       "       ...,\n",
       "       [-2764198.6820607 ],\n",
       "       [-2764198.68206071],\n",
       "       [33730980.31793932]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reducing the number of components to 3 using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "data_pca = pca.fit_transform(X)\n",
    "data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.764199e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.660609e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.764199e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.762507e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.626609e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>-2.764199e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>-2.764199e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>-2.764199e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>-2.764199e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>3.373098e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34299 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                PC1\n",
       "0     -2.764199e+06\n",
       "1     -2.660609e+06\n",
       "2     -2.764199e+06\n",
       "3     -2.762507e+06\n",
       "4     -2.626609e+06\n",
       "...             ...\n",
       "34294 -2.764199e+06\n",
       "34295 -2.764199e+06\n",
       "34296 -2.764199e+06\n",
       "34297 -2.764199e+06\n",
       "34298  3.373098e+07\n",
       "\n",
       "[34299 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a DatFrame\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1'])\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(df_pca, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X_train and X_test\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler1 = scaler.fit(X_train1)\n",
    "\n",
    "# Scale the data\n",
    "X_train1_scaled = X_scaler1.transform(X_train)\n",
    "X_test1_scaled = X_scaler1.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train1_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - one layer neural net\n",
    "number_input_features = len(X_train1_scaled[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 6\n",
    "# hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.2661 - acc: 0.5257\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2492 - acc: 0.5351\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2492 - acc: 0.5360\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2497 - acc: 0.5344\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2492 - acc: 0.5334\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2492 - acc: 0.5340\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2499 - acc: 0.5314\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5337\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2501 - acc: 0.5326\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2491 - acc: 0.5303\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.2491 - acc: 0.5354\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.2491 - acc: 0.5351\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2505 - acc: 0.5338\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5356\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5347\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2498 - acc: 0.5339\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5354\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2492 - acc: 0.5311\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.2497 - acc: 0.5341\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.2494 - acc: 0.5348\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.2491 - acc: 0.5347\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2491 - acc: 0.5350\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2491 - acc: 0.5350\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2490 - acc: 0.5344\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2498 - acc: 0.5340\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2490 - acc: 0.5342\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5339\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5342\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2490 - acc: 0.5344\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2491 - acc: 0.5316\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2491 - acc: 0.5325\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5334\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5346\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2490 - acc: 0.5329\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5337\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5346\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5328\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5336\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5344\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2499 - acc: 0.5343\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2490 - acc: 0.5353\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.2490 - acc: 0.5348\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.2489 - acc: 0.5335\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5337\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5350\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2488 - acc: 0.5335\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2490 - acc: 0.5346\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.2489 - acc: 0.5327\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5337\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5338\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.53490s - loss: 0.\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5348\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5351\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5350\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.2489 - acc: 0.5347\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2490 - acc: 0.5327\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.2489 - acc: 0.5341\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5342\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5337\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.2489 - acc: 0.5346\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5350\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5347\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2495 - acc: 0.5348\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5344\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5347\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5333\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5348\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2489 - acc: 0.5348\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.2489 - acc: 0.5346\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5351\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5335\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5351\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.2490 - acc: 0.5347\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5349\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5332\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5353\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2488 - acc: 0.5350\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2489 - acc: 0.5334\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2488 - acc: 0.5353\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2489 - acc: 0.5351\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.2489 - acc: 0.5334\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2489 - acc: 0.5348\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.2489 - acc: 0.5351\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2489 - acc: 0.5348\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2488 - acc: 0.5349\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2489 - acc: 0.5352\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.2489 - acc: 0.5351\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5349\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5344\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5351\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.2490 - acc: 0.5345\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.2489 - acc: 0.5346\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.2489 - acc: 0.5345\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5344\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.2489 - acc: 0.5341\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2488 - acc: 0.5348\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2490 - acc: 0.5349\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5349\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5348\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.2489 - acc: 0.5348\n",
      "8575/8575 - 0s - loss: 0.2487 - acc: 0.5368\n",
      "Loss: 0.24865732516039912, Accuracy: 0.5367929935455322\n"
     ]
    }
   ],
   "source": [
    "# Train the model with 100 epochs\n",
    "fit_model = nn.fit(X_train1_scaled, y_train1, epochs=100) \n",
    "# Evaluate the model using the test data \n",
    "model_loss, model_accuracy = nn.evaluate(X_test1_scaled,y_test1,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It didn't improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how Random forest and Logistic Regression work on this data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 12035, 1: 13689})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data.\n",
    "predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>455</td>\n",
       "      <td>3548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>557</td>\n",
       "      <td>4015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          455         3548\n",
       "Actual 1          557         4015"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5212827988338192"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>455</td>\n",
       "      <td>3548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>557</td>\n",
       "      <td>4015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          455         3548\n",
       "Actual 1          557         4015"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.5212827988338192\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.11      0.18      4003\n",
      "           1       0.53      0.88      0.66      4572\n",
      "\n",
      "    accuracy                           0.52      8575\n",
      "   macro avg       0.49      0.50      0.42      8575\n",
      "weighted avg       0.49      0.52      0.44      8575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=78)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=78)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train_scaled, y_train)\n",
    "y_pred = classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5, 3998],\n",
       "       [   0, 4572]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.500624531601299"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neither Random Forest or Logistic Regression does better job. So even though the neural network and deep learning result is not reaching 75%, it is still better than these two models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
